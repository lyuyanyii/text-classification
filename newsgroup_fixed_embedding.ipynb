{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPalgfi0XBGgeQ3Ie65qaOa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyuyanyii/text-classification/blob/main/newsgroup_fixed_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V6W6m5pdPKt",
        "outputId": "54b7856e-20d3-451e-b9c3-0847e900422b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.24702078]\n",
            " [ 0.2837508 ]\n",
            " [-1.47260885]\n",
            " [-0.13748325]\n",
            " [-0.62559477]\n",
            " [-1.01040063]\n",
            " [ 0.85770094]\n",
            " [ 0.56877155]\n",
            " [-0.32867668]\n",
            " [ 2.63014969]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(np.random.normal(loc = 0, scale = 1, size = (10, 1)))\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.system('wget https://github.com/lil-lab/lm-class/raw/refs/heads/main/assignments/a1/starter-repo/data/newsgroups/train/train_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.system('wget https://github.com/lil-lab/lm-class/raw/refs/heads/main/assignments/a1/starter-repo/data/newsgroups/train/train_labels.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMpukklYa9Cq",
        "outputId": "f39f9342-b47b-4543-a70b-2018b9c22bba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import string\n",
        "\n",
        "D = 256\n",
        "\n",
        "def data_process(raw_text : string):\n",
        "    text = raw_text.lower()\n",
        "    raw_word_list = text.split()\n",
        "    word_list = []\n",
        "    for raw_word in raw_word_list:\n",
        "        word = ''.join([i for i in raw_word if i.isalpha()])\n",
        "        if word != '':\n",
        "            word_list.append(word)\n",
        "    return word_list\n",
        "\n",
        "def bag_of_words_embedding(word_vector_dict : dict, word_list : list):\n",
        "    vector_list = []\n",
        "\n",
        "    for word in word_list:\n",
        "        if word not in word_vector_dict:\n",
        "            word_vector_dict[word] = np.random.normal(loc = 0, scale = 0.5, size = (D, 1))\n",
        "\n",
        "        embedding = word_vector_dict[word]\n",
        "        vector_list.append(embedding)\n",
        "\n",
        "    bag_of_words = np.concatenate(vector_list, axis = 1).mean(axis = 1)\n",
        "    return bag_of_words\n",
        "\n",
        "train_data = []\n",
        "word_vector_dict = {}\n",
        "\n",
        "with open('train_data.csv', newline = '') as file:\n",
        "    csv_file = csv.DictReader(file)\n",
        "    for line in csv_file:\n",
        "        word_list = data_process(line['text'])\n",
        "\n",
        "        train_data.append(bag_of_words_embedding(word_vector_dict, word_list))\n",
        "\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGfl94rwfAH8",
        "outputId": "6d1074e7-b7d3-4ed1-a451-8c2903f7adea"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "label_index_map = {}\n",
        "\n",
        "num_class = 0\n",
        "\n",
        "with open('train_labels.csv', newline = '') as file:\n",
        "    csv_file = csv.DictReader(file)\n",
        "    for line in csv_file:\n",
        "        label = line['newsgroup']\n",
        "\n",
        "        if label not in label_index_map:\n",
        "            label_index_map[label] = num_class\n",
        "            num_class += 1\n",
        "\n",
        "        index = label_index_map[label]\n",
        "        train_labels.append(index)\n",
        "\n",
        "print(num_class)\n",
        "print(len(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJnrLn50iRxE",
        "outputId": "37609ca8-3170-46ca-e7c9-9f8afb7d90fb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "9051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "dataset_size = len(train_data)\n",
        "\n",
        "valid_data = train_data[ int(dataset_size * train_ratio) : ]\n",
        "valid_labels = train_labels[ int(dataset_size * train_ratio) : ]\n",
        "\n",
        "train_data = train_data[ : int(dataset_size * train_ratio) ]\n",
        "train_labels = train_labels[ : int(dataset_size * train_ratio) ]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SddOeqd3lniy",
        "outputId": "2f144d2e-5c28-4c3c-c3ab-699315cc745f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7240\n",
            "1811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        self.w = np.random.normal(loc = 0, scale = 1 / in_dim**0.5, size = (out_dim, in_dim))\n",
        "        self.b = np.random.normal(loc = 0, scale = 1 / in_dim**0.5, size = (out_dim, 1))\n",
        "        self.grad_w = np.zeros((out_dim, in_dim))\n",
        "        self.grad_b = np.zeros((out_dim, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        self.out = np.matmul(self.w, x) + self.b\n",
        "        return self.out\n",
        "\n",
        "    def back_prop(self, grad_out):\n",
        "        self.grad_b = grad_out.mean(axis = 0)\n",
        "        self.grad_w = np.matmul(grad_out, self.x.transpose(0, 2, 1)).mean(axis = 0)\n",
        "        self.grad_x = np.matmul(self.w.transpose((1, 0)), grad_out)\n",
        "        return self.grad_x\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.w += self.grad_w * lr\n",
        "        self.b += self.grad_b * lr\n"
      ],
      "metadata": {
        "id": "WPSPkyIMpO4_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x >= 0)\n",
        "        return x * self.mask\n",
        "\n",
        "    def back_prop(self, grad_y):\n",
        "        return grad_y * self.mask\n"
      ],
      "metadata": {
        "id": "ASRyBfc00flJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        exp_x = np.exp(x)\n",
        "        d1, d2, d3 = x.shape\n",
        "        self.y = exp_x / exp_x.sum(axis = 1).reshape(d1, 1, 1)\n",
        "        return self.y\n",
        "\n",
        "    def back_prop(self, grad_y):\n",
        "        (d1, d2, d3) = grad_y.shape\n",
        "        # print((np.identity(d2) - self.y).shape)\n",
        "        # print((self.y.transpose(0, 2, 1)* (np.identity(d2, ) - self.y)).shape)\n",
        "        # print(self.y.transpose(0, 2, 1))\n",
        "        # print(self.y.transpose(0, 2, 1)* (np.identity(d2, ) - self.y))\n",
        "        grad_x = np.matmul( (self.y.transpose(0, 2, 1) * (np.identity(d2) - self.y)), grad_y )\n",
        "        return grad_x\n",
        "\n",
        "\"\"\"\n",
        "x = np.arange(0, 3)\n",
        "x = x.reshape((1, 3, 1))\n",
        "sft_layer = Softmax()\n",
        "y = sft_layer.forward(x)\n",
        "print(y)\n",
        "grad_y = np.array([1,0,0])\n",
        "grad_y = grad_y.reshape((1, 3, 1))\n",
        "grad_x = sft_layer.back_prop(grad_y)\n",
        "print(grad_x)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZAU3E9sN1acV",
        "outputId": "909fc464-57a9-410c-c676-05d934231431"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nx = np.arange(0, 3)\\nx = x.reshape((1, 3, 1))\\nsft_layer = Softmax()\\ny = sft_layer.forward(x)\\nprint(y)\\ngrad_y = np.array([1,0,0])\\ngrad_y = grad_y.reshape((1, 3, 1))\\ngrad_x = sft_layer.back_prop(grad_y)\\nprint(grad_x)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "depth = 4\n",
        "\n",
        "network = []\n",
        "for i in range(depth - 1):\n",
        "    network.append((Linear(D, D), Relu()))\n",
        "network.append((Linear(D, num_class), Softmax()))\n",
        "\n",
        "def train(network, data, labels, lr):\n",
        "    x = np.vstack(data)\n",
        "    (d1, d2) = x.shape\n",
        "    x = x.reshape(d1, d2, 1)\n",
        "    for (layer, f_layer) in network:\n",
        "        x = f_layer.forward(layer.forward(x))\n",
        "\n",
        "    # loss1 = - np.log(np.vstack([x[i][labels[i]][0] for i in range(d1)]))\n",
        "    # print(labels)\n",
        "    d1, d2, d3 = x.shape\n",
        "    loss = - np.log(x[range(d1), labels, 0]).mean()\n",
        "    grad_x = np.zeros((d1, d2, d3))\n",
        "    for i in range(d1):\n",
        "        grad_x[i, labels[i], 0] = 1/x[i, labels[i], 0]\n",
        "\n",
        "    for (layer, f_layer) in reversed(network):\n",
        "        grad_y1 = f_layer.back_prop(grad_x)\n",
        "        grad_y2 = layer.back_prop(grad_y1)\n",
        "        grad_x = grad_y2\n",
        "        layer.update(lr)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def eval(network, data):\n",
        "    x = np.vstack(data)\n",
        "    (d1, d2) = x.shape\n",
        "    x = x.reshape(d1, d2, 1)\n",
        "    for (layer, f_layer) in network:\n",
        "        x = f_layer.forward(layer.forward(x))\n",
        "\n",
        "    d1, d2, d3 = x.shape\n",
        "    x = x.reshape(d1, d2)\n",
        "    idx = np.argmax(x, axis = 1)\n",
        "    return idx\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    loss = 0\n",
        "    print('---- Epoch ', i, '----')\n",
        "    for j in range(len(train_data) // 10):\n",
        "        start = j * 10\n",
        "        end = min(start + 10, len(train_data))\n",
        "        loss = train(network, train_data[start:end], train_labels[start:end], lr = 0.1 * (num_epochs - i) / num_epochs + 0.01)\n",
        "    print('Loss = ', loss)\n",
        "\n",
        "    pred = eval(network, valid_data)\n",
        "    acc = np.array(pred == valid_labels).mean()\n",
        "    print('Validation Accurary = ', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1G9mk2zG76a",
        "outputId": "404ae150-38db-4f28-e65c-001178ceaeb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Epoch  0 ----\n",
            "Loss =  2.45418784863963\n",
            "Validation Accurary =  0.12147984538928769\n",
            "---- Epoch  1 ----\n",
            "Loss =  2.2338815837316113\n",
            "Validation Accurary =  0.20927664273881832\n",
            "---- Epoch  2 ----\n",
            "Loss =  1.9100403334549079\n",
            "Validation Accurary =  0.2921038100496963\n",
            "---- Epoch  3 ----\n",
            "Loss =  1.7830872234689163\n",
            "Validation Accurary =  0.3412479293208172\n",
            "---- Epoch  4 ----\n",
            "Loss =  1.617977386149852\n",
            "Validation Accurary =  0.36830480397570403\n",
            "---- Epoch  5 ----\n",
            "Loss =  1.499001374952885\n",
            "Validation Accurary =  0.3887355052457206\n",
            "---- Epoch  6 ----\n",
            "Loss =  1.3985067366380104\n",
            "Validation Accurary =  0.4075096631695196\n",
            "---- Epoch  7 ----\n",
            "Loss =  1.3566296527144244\n",
            "Validation Accurary =  0.4196576477084484\n",
            "---- Epoch  8 ----\n",
            "Loss =  1.2666235795771879\n",
            "Validation Accurary =  0.4389839867476532\n",
            "---- Epoch  9 ----\n",
            "Loss =  1.412534130419577\n",
            "Validation Accurary =  0.45389287686361124\n",
            "---- Epoch  10 ----\n",
            "Loss =  1.1607275578385745\n",
            "Validation Accurary =  0.46383213694091663\n",
            "---- Epoch  11 ----\n",
            "Loss =  1.1323638391579764\n",
            "Validation Accurary =  0.4583103257868581\n",
            "---- Epoch  12 ----\n",
            "Loss =  1.2042766430697127\n",
            "Validation Accurary =  0.496410822749862\n",
            "---- Epoch  13 ----\n",
            "Loss =  0.9207911339305671\n",
            "Validation Accurary =  0.47432357813362785\n",
            "---- Epoch  14 ----\n",
            "Loss =  1.2738578364620008\n",
            "Validation Accurary =  0.48812810601877415\n",
            "---- Epoch  15 ----\n",
            "Loss =  0.8869640737533713\n",
            "Validation Accurary =  0.48591938155715075\n",
            "---- Epoch  16 ----\n",
            "Loss =  0.6885520320771418\n",
            "Validation Accurary =  0.45996686913307566\n",
            "---- Epoch  17 ----\n",
            "Loss =  0.9384117896530915\n",
            "Validation Accurary =  0.49751518498067365\n",
            "---- Epoch  18 ----\n",
            "Loss =  0.41018701283147274\n",
            "Validation Accurary =  0.4803975704030922\n",
            "---- Epoch  19 ----\n",
            "Loss =  0.46605965801132276\n",
            "Validation Accurary =  0.48371065709552735\n",
            "---- Epoch  20 ----\n",
            "Loss =  0.565013607375491\n",
            "Validation Accurary =  0.47156267255659856\n",
            "---- Epoch  21 ----\n",
            "Loss =  0.34141626990134155\n",
            "Validation Accurary =  0.47653230259525126\n",
            "---- Epoch  22 ----\n",
            "Loss =  0.26250721460679005\n",
            "Validation Accurary =  0.47377139701822196\n",
            "---- Epoch  23 ----\n",
            "Loss =  0.08641161682121229\n",
            "Validation Accurary =  0.47763666482606293\n",
            "---- Epoch  24 ----\n",
            "Loss =  0.5110206759364246\n",
            "Validation Accurary =  0.45775814467145226\n",
            "---- Epoch  25 ----\n",
            "Loss =  0.05032467366898373\n",
            "Validation Accurary =  0.4870237437879624\n",
            "---- Epoch  26 ----\n",
            "Loss =  0.10073542272897049\n",
            "Validation Accurary =  0.45720596355604637\n",
            "---- Epoch  27 ----\n",
            "Loss =  0.041580431966738565\n",
            "Validation Accurary =  0.5019326339039205\n",
            "---- Epoch  28 ----\n",
            "Loss =  0.3180821286239045\n",
            "Validation Accurary =  0.4870237437879624\n",
            "---- Epoch  29 ----\n",
            "Loss =  0.15652746542222223\n",
            "Validation Accurary =  0.48868028713418\n",
            "---- Epoch  30 ----\n",
            "Loss =  0.2170283716100858\n",
            "Validation Accurary =  0.47874102705687466\n",
            "---- Epoch  31 ----\n",
            "Loss =  0.28528785354851166\n",
            "Validation Accurary =  0.484815019326339\n",
            "---- Epoch  32 ----\n",
            "Loss =  0.07916321454605595\n",
            "Validation Accurary =  0.48812810601877415\n",
            "---- Epoch  33 ----\n",
            "Loss =  0.2776950240801281\n",
            "Validation Accurary =  0.500276090557703\n",
            "---- Epoch  34 ----\n",
            "Loss =  0.07267763836339308\n",
            "Validation Accurary =  0.5057979017117614\n",
            "---- Epoch  35 ----\n",
            "Loss =  0.13627341783933677\n",
            "Validation Accurary =  0.5041413583655439\n",
            "---- Epoch  36 ----\n",
            "Loss =  0.136718608216234\n",
            "Validation Accurary =  0.5019326339039205\n",
            "---- Epoch  37 ----\n",
            "Loss =  0.008846362814812088\n",
            "Validation Accurary =  0.4826062948647156\n",
            "---- Epoch  38 ----\n",
            "Loss =  0.02832060334328061\n",
            "Validation Accurary =  0.4980673660960795\n",
            "---- Epoch  39 ----\n",
            "Loss =  0.02443095263655672\n",
            "Validation Accurary =  0.518498067366096\n",
            "---- Epoch  40 ----\n",
            "Loss =  0.00464584483081174\n",
            "Validation Accurary =  0.5151849806736609\n",
            "---- Epoch  41 ----\n",
            "Loss =  0.263673305409782\n",
            "Validation Accurary =  0.5019326339039205\n",
            "---- Epoch  42 ----\n",
            "Loss =  0.021704564890748394\n",
            "Validation Accurary =  0.5267807840971839\n",
            "---- Epoch  43 ----\n",
            "Loss =  0.008865663179593586\n",
            "Validation Accurary =  0.5339591385974599\n",
            "---- Epoch  44 ----\n",
            "Loss =  0.034077099315304324\n",
            "Validation Accurary =  0.5179458862506903\n",
            "---- Epoch  45 ----\n",
            "Loss =  0.008967481005948928\n",
            "Validation Accurary =  0.5378244064053009\n",
            "---- Epoch  46 ----\n",
            "Loss =  0.04393061550905595\n",
            "Validation Accurary =  0.5284373274434014\n",
            "---- Epoch  47 ----\n",
            "Loss =  0.007902493560077323\n",
            "Validation Accurary =  0.5394809497515185\n",
            "---- Epoch  48 ----\n",
            "Loss =  0.010783929754447205\n",
            "Validation Accurary =  0.5405853119823302\n",
            "---- Epoch  49 ----\n",
            "Loss =  0.007342750917471827\n",
            "Validation Accurary =  0.5389287686361126\n",
            "---- Epoch  50 ----\n",
            "Loss =  0.005383087270984747\n",
            "Validation Accurary =  0.5350635008282717\n",
            "---- Epoch  51 ----\n",
            "Loss =  0.004645807250069499\n",
            "Validation Accurary =  0.537272225289895\n",
            "---- Epoch  52 ----\n",
            "Loss =  0.004221956636426541\n",
            "Validation Accurary =  0.5394809497515185\n",
            "---- Epoch  53 ----\n",
            "Loss =  0.0034427574514035676\n",
            "Validation Accurary =  0.5389287686361126\n",
            "---- Epoch  54 ----\n",
            "Loss =  0.0029160336364811882\n",
            "Validation Accurary =  0.5389287686361126\n",
            "---- Epoch  55 ----\n",
            "Loss =  0.0025165193077770144\n",
            "Validation Accurary =  0.5405853119823302\n",
            "---- Epoch  56 ----\n",
            "Loss =  0.002272192535808238\n",
            "Validation Accurary =  0.5427940364439536\n",
            "---- Epoch  57 ----\n",
            "Loss =  0.0021354162018077027\n",
            "Validation Accurary =  0.5427940364439536\n",
            "---- Epoch  58 ----\n",
            "Loss =  0.0019895145821882884\n",
            "Validation Accurary =  0.5427940364439536\n",
            "---- Epoch  59 ----\n",
            "Loss =  0.0018283708474771134\n",
            "Validation Accurary =  0.5455549420209829\n",
            "---- Epoch  60 ----\n",
            "Loss =  0.00174840046898815\n",
            "Validation Accurary =  0.5444505797901712\n",
            "---- Epoch  61 ----\n",
            "Loss =  0.0016213899356902935\n",
            "Validation Accurary =  0.5438983986747653\n",
            "---- Epoch  62 ----\n",
            "Loss =  0.001499137334890604\n",
            "Validation Accurary =  0.5444505797901712\n",
            "---- Epoch  63 ----\n",
            "Loss =  0.0014492805097930067\n",
            "Validation Accurary =  0.545002760905577\n",
            "---- Epoch  64 ----\n",
            "Loss =  0.001382069398019512\n",
            "Validation Accurary =  0.5461071231363888\n",
            "---- Epoch  65 ----\n",
            "Loss =  0.001294361310370323\n",
            "Validation Accurary =  0.5455549420209829\n",
            "---- Epoch  66 ----\n",
            "Loss =  0.0012096324016457015\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  67 ----\n",
            "Loss =  0.0011317933718828334\n",
            "Validation Accurary =  0.5494202098288239\n",
            "---- Epoch  68 ----\n",
            "Loss =  0.0011421480607412514\n",
            "Validation Accurary =  0.5494202098288239\n",
            "---- Epoch  69 ----\n",
            "Loss =  0.0011318065278166374\n",
            "Validation Accurary =  0.5483158475980121\n",
            "---- Epoch  70 ----\n",
            "Loss =  0.0010614722565984573\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  71 ----\n",
            "Loss =  0.0010587068837538888\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  72 ----\n",
            "Loss =  0.0010255303182108862\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  73 ----\n",
            "Loss =  0.000986317123748985\n",
            "Validation Accurary =  0.5466593042517945\n",
            "---- Epoch  74 ----\n",
            "Loss =  0.0009988138411381492\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  75 ----\n",
            "Loss =  0.000958450056820013\n",
            "Validation Accurary =  0.5461071231363888\n",
            "---- Epoch  76 ----\n",
            "Loss =  0.0009447919196561522\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  77 ----\n",
            "Loss =  0.0009395629622988621\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  78 ----\n",
            "Loss =  0.0009303223782418203\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  79 ----\n",
            "Loss =  0.000913287289554431\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  80 ----\n",
            "Loss =  0.0008951147254776653\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  81 ----\n",
            "Loss =  0.0008922423281970696\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  82 ----\n",
            "Loss =  0.0008655420469994297\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  83 ----\n",
            "Loss =  0.0008601183778969151\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  84 ----\n",
            "Loss =  0.0008541169015765376\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  85 ----\n",
            "Loss =  0.0008425876150524216\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  86 ----\n",
            "Loss =  0.0008380730605762304\n",
            "Validation Accurary =  0.5477636664826063\n",
            "---- Epoch  87 ----\n",
            "Loss =  0.0008298649260202986\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  88 ----\n",
            "Loss =  0.0008210092392349743\n",
            "Validation Accurary =  0.5472114853672004\n",
            "---- Epoch  89 ----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SFZe8By_D997"
      }
    }
  ]
}