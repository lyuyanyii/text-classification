{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8DAraux0vdHQzWCYsoVvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyuyanyii/text-classification/blob/main/newsgroup_trained_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V6W6m5pdPKt",
        "outputId": "f36386c8-f9bb-4d62-c37b-c134b5d2b2bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.system('wget https://github.com/lil-lab/lm-class/raw/refs/heads/main/assignments/a1/starter-repo/data/newsgroups/train/train_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.system('wget https://github.com/lil-lab/lm-class/raw/refs/heads/main/assignments/a1/starter-repo/data/newsgroups/train/train_labels.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMpukklYa9Cq",
        "outputId": "dd0d30a7-612f-466b-da66-fff9eb75c981"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import string\n",
        "\n",
        "D = 128\n",
        "\n",
        "def data_process(raw_text : string):\n",
        "    text = raw_text.lower()\n",
        "    raw_word_list = text.split()\n",
        "    word_list = []\n",
        "    for raw_word in raw_word_list:\n",
        "        word = ''.join([i for i in raw_word if i.isalpha()])\n",
        "        if word != '':\n",
        "            word_list.append(word)\n",
        "    return word_list\n",
        "\n",
        "\"\"\"\n",
        "def bag_of_words_embedding(word_vector_dict : dict, word_list : list):\n",
        "    vector_list = []\n",
        "\n",
        "    for word in word_list:\n",
        "        if word not in word_vector_dict:\n",
        "            word_vector_dict[word] = np.random.normal(loc = 0, scale = 0.5, size = (D, 1))\n",
        "\n",
        "        embedding = word_vector_dict[word]\n",
        "        vector_list.append(embedding)\n",
        "\n",
        "    bag_of_words = np.concatenate(vector_list, axis = 1).mean(axis = 1)\n",
        "    return bag_of_words\n",
        "\"\"\"\n",
        "\n",
        "train_data = []\n",
        "# word_vector_dict = {}\n",
        "\n",
        "with open('train_data.csv', newline = '') as file:\n",
        "    csv_file = csv.DictReader(file)\n",
        "    for line in csv_file:\n",
        "        word_list = data_process(line['text'])\n",
        "\n",
        "        train_data.append(word_list)\n",
        "\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGfl94rwfAH8",
        "outputId": "1db546a6-7de3-4726-d002-cfcad58a2c43"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word_Embedding:\n",
        "    def __init__(self, dataset):\n",
        "        self.embedding = {}\n",
        "\n",
        "        for text in dataset:\n",
        "            for word in text:\n",
        "                if word not in self.embedding:\n",
        "                    self.embedding[word] = np.random.normal(loc = 0, scale = len(text)**0.5, size = (D, 1))\n",
        "\n",
        "    def forward(self, text_batch):\n",
        "        self.x = []\n",
        "        self.text_batch = text_batch\n",
        "        for text in text_batch:\n",
        "            bag_of_words = []\n",
        "            for word in text:\n",
        "                bag_of_words.append( self.embedding[word] )\n",
        "            bag_of_words = np.concatenate( bag_of_words, axis = 1).mean(axis = 1)\n",
        "            bag_of_words = bag_of_words.reshape((1, bag_of_words.shape[0], 1))\n",
        "            self.x.append( bag_of_words )\n",
        "        self.x = np.concatenate(self.x, axis = 0)\n",
        "        return self.x\n",
        "\n",
        "    def back_prop(self, grad_x):\n",
        "        self.grad_embedding = {}\n",
        "\n",
        "        for i in range(len(text_batch)):\n",
        "            text = text_batch[i]\n",
        "\n",
        "            for word in text:\n",
        "                if word not in self.grad_embedding:\n",
        "                    self.grad_embedding[word] = np.zeros((D, 1))\n",
        "                self.grad_embedding[word] += grad_x[i] / len(text) / len(text_batch)\n",
        "\n",
        "    def update(self, lr):\n",
        "        for (word, grad) in self.grad_embedding.items():\n",
        "            self.embedding[word] += grad * lr\n"
      ],
      "metadata": {
        "id": "hjBDmNZ7x9cw"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "label_index_map = {}\n",
        "\n",
        "num_class = 0\n",
        "\n",
        "with open('train_labels.csv', newline = '') as file:\n",
        "    csv_file = csv.DictReader(file)\n",
        "    for line in csv_file:\n",
        "        label = line['newsgroup']\n",
        "\n",
        "        if label not in label_index_map:\n",
        "            label_index_map[label] = num_class\n",
        "            num_class += 1\n",
        "\n",
        "        index = label_index_map[label]\n",
        "        train_labels.append(index)\n",
        "\n",
        "print(num_class)\n",
        "print(len(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJnrLn50iRxE",
        "outputId": "aa546fc4-5c4b-45be-defa-d012b33e57aa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "9051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "dataset_size = len(train_data)\n",
        "\n",
        "valid_data = train_data[ int(dataset_size * train_ratio) : ]\n",
        "valid_labels = train_labels[ int(dataset_size * train_ratio) : ]\n",
        "\n",
        "train_data = train_data[ : int(dataset_size * train_ratio) ]\n",
        "train_labels = train_labels[ : int(dataset_size * train_ratio) ]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SddOeqd3lniy",
        "outputId": "6f741269-aaed-47a9-e7b3-6f151ce4a67c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7240\n",
            "1811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        self.w = np.random.normal(loc = 0, scale = 1 / in_dim**0.5, size = (out_dim, in_dim))\n",
        "        self.b = np.random.normal(loc = 0, scale = 1 / in_dim**0.5, size = (out_dim, 1))\n",
        "        self.grad_w = np.zeros((out_dim, in_dim))\n",
        "        self.grad_b = np.zeros((out_dim, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        self.out = np.matmul(self.w, x) + self.b\n",
        "        return self.out\n",
        "\n",
        "    def back_prop(self, grad_out):\n",
        "        self.grad_b = grad_out.mean(axis = 0)\n",
        "        self.grad_w = np.matmul(grad_out, self.x.transpose(0, 2, 1)).mean(axis = 0)\n",
        "        self.grad_x = np.matmul(self.w.transpose((1, 0)), grad_out)\n",
        "        return self.grad_x\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.w += self.grad_w * lr\n",
        "        self.b += self.grad_b * lr\n"
      ],
      "metadata": {
        "id": "WPSPkyIMpO4_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x >= 0)\n",
        "        return x * self.mask\n",
        "\n",
        "    def back_prop(self, grad_y):\n",
        "        return grad_y * self.mask\n"
      ],
      "metadata": {
        "id": "ASRyBfc00flJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        exp_x = np.exp(x)\n",
        "        d1, d2, d3 = x.shape\n",
        "        self.y = exp_x / exp_x.sum(axis = 1).reshape(d1, 1, 1)\n",
        "        return self.y\n",
        "\n",
        "    def back_prop(self, grad_y):\n",
        "        (d1, d2, d3) = grad_y.shape\n",
        "        # print((np.identity(d2) - self.y).shape)\n",
        "        # print((self.y.transpose(0, 2, 1)* (np.identity(d2, ) - self.y)).shape)\n",
        "        # print(self.y.transpose(0, 2, 1))\n",
        "        # print(self.y.transpose(0, 2, 1)* (np.identity(d2, ) - self.y))\n",
        "        grad_x = np.matmul( (self.y.transpose(0, 2, 1) * (np.identity(d2) - self.y)), grad_y )\n",
        "        return grad_x\n",
        "\n",
        "\"\"\"\n",
        "x = np.arange(0, 3)\n",
        "x = x.reshape((1, 3, 1))\n",
        "sft_layer = Softmax()\n",
        "y = sft_layer.forward(x)\n",
        "print(y)\n",
        "grad_y = np.array([1,0,0])\n",
        "grad_y = grad_y.reshape((1, 3, 1))\n",
        "grad_x = sft_layer.back_prop(grad_y)\n",
        "print(grad_x)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZAU3E9sN1acV",
        "outputId": "13f04821-ff56-4491-f779-3d5bf9335914"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nx = np.arange(0, 3)\\nx = x.reshape((1, 3, 1))\\nsft_layer = Softmax()\\ny = sft_layer.forward(x)\\nprint(y)\\ngrad_y = np.array([1,0,0])\\ngrad_y = grad_y.reshape((1, 3, 1))\\ngrad_x = sft_layer.back_prop(grad_y)\\nprint(grad_x)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "depth = 3\n",
        "\n",
        "embedding_layer = Word_Embedding(train_data + valid_data)\n",
        "network = []\n",
        "for i in range(depth - 1):\n",
        "    network.append((Linear(D, D), Relu()))\n",
        "network.append((Linear(D, num_class), Softmax()))\n",
        "\n",
        "def train(embedding_layer, network, data, labels, lr):\n",
        "    x = embedding_layer.forward(data)\n",
        "    for (layer, f_layer) in network:\n",
        "        x = f_layer.forward(layer.forward(x))\n",
        "\n",
        "    # loss1 = - np.log(np.vstack([x[i][labels[i]][0] for i in range(d1)]))\n",
        "    d1, d2, d3 = x.shape\n",
        "    loss = - np.log(x[range(d1), labels, 0]).mean()\n",
        "    grad_x = np.zeros((d1, d2, d3))\n",
        "    for i in range(d1):\n",
        "        grad_x[i, labels[i], 0] = 1/x[i, labels[i], 0]\n",
        "\n",
        "    for (layer, f_layer) in reversed(network):\n",
        "        grad_y1 = f_layer.back_prop(grad_x)\n",
        "        grad_y2 = layer.back_prop(grad_y1)\n",
        "        grad_x = grad_y2\n",
        "        layer.update(lr)\n",
        "\n",
        "    embedding_layer.back_prop(grad_x)\n",
        "    embedding_layer.update(lr)\n",
        "\n",
        "    x = x.reshape(d1, d2)\n",
        "    idx = np.argmax(x, axis = 1)\n",
        "    acc = np.array(idx == labels).mean()\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def eval(embedding_layer, network, data):\n",
        "    x = embedding_layer.forward(data)\n",
        "    for (layer, f_layer) in network:\n",
        "        x = f_layer.forward(layer.forward(x))\n",
        "\n",
        "    d1, d2, d3 = x.shape\n",
        "    x = x.reshape(d1, d2)\n",
        "    idx = np.argmax(x, axis = 1)\n",
        "    return idx\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    loss = 0\n",
        "\n",
        "    idx = list(np.random.permutation(len(train_data)))\n",
        "    train_data = [train_data[i] for i in idx]\n",
        "    train_labels = [train_labels[i] for i in idx]\n",
        "\n",
        "    print('---- Epoch ', i, '----')\n",
        "    acc_list = []\n",
        "    loss_list = []\n",
        "    for j in range(len(train_data) // batch_size):\n",
        "        start = j * batch_size\n",
        "        end = min(start + batch_size, len(train_data))\n",
        "        loss, acc = train(embedding_layer, network, train_data[start:end], train_labels[start:end], lr = 0.1 * (num_epochs - i) / num_epochs + 0.01)\n",
        "        acc_list.append(acc)\n",
        "        loss_list.append(loss)\n",
        "    print('Loss =', np.array(loss_list).mean(), ', Training Acc =', np.array(acc_list).mean())\n",
        "\n",
        "    pred = eval(embedding_layer, network, valid_data)\n",
        "    acc = np.array(pred == valid_labels).mean()\n",
        "    print('Validation Accurary = ', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1G9mk2zG76a",
        "outputId": "ca366599-e1e5-431e-b36c-54b27163cdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Epoch  0 ----\n",
            "Loss = 2.9666647732208076 , Training Acc = 0.08038674033149171\n",
            "Validation Accurary =  0.09939260077305356\n",
            "---- Epoch  1 ----\n",
            "Loss = 2.806166190511195 , Training Acc = 0.12582872928176794\n",
            "Validation Accurary =  0.13694091662065158\n",
            "---- Epoch  2 ----\n",
            "Loss = 2.6469282555539837 , Training Acc = 0.1743093922651934\n",
            "Validation Accurary =  0.18553285477636663\n",
            "---- Epoch  3 ----\n",
            "Loss = 2.5192083602466586 , Training Acc = 0.22168508287292818\n",
            "Validation Accurary =  0.22473771397018222\n",
            "---- Epoch  4 ----\n",
            "Loss = 2.4050731464862296 , Training Acc = 0.26160220994475136\n",
            "Validation Accurary =  0.26339039204859194\n",
            "---- Epoch  5 ----\n",
            "Loss = 2.3229607754373385 , Training Acc = 0.28604972375690607\n",
            "Validation Accurary =  0.27277747101049143\n",
            "---- Epoch  6 ----\n",
            "Loss = 2.235444875434165 , Training Acc = 0.313121546961326\n",
            "Validation Accurary =  0.29044726670347876\n",
            "---- Epoch  7 ----\n",
            "Loss = 2.178817147145349 , Training Acc = 0.33259668508287293\n",
            "Validation Accurary =  0.2887907233572612\n",
            "---- Epoch  8 ----\n",
            "Loss = 2.1267205046606543 , Training Acc = 0.35220994475138123\n",
            "Validation Accurary =  0.308117062396466\n",
            "---- Epoch  9 ----\n",
            "Loss = 2.0961547169864225 , Training Acc = 0.3611878453038674\n",
            "Validation Accurary =  0.293208172280508\n",
            "---- Epoch  10 ----\n",
            "Loss = 2.0803174184819158 , Training Acc = 0.3680939226519337\n",
            "Validation Accurary =  0.3097736057426836\n",
            "---- Epoch  11 ----\n",
            "Loss = 2.0316901290221545 , Training Acc = 0.37803867403314906\n",
            "Validation Accurary =  0.33296521258972944\n",
            "---- Epoch  12 ----\n",
            "Loss = 1.9847890694120534 , Training Acc = 0.39392265193370174\n",
            "Validation Accurary =  0.33020430701270015\n",
            "---- Epoch  13 ----\n",
            "Loss = 1.9538340230791174 , Training Acc = 0.4037292817679558\n",
            "Validation Accurary =  0.32081722805080065\n",
            "---- Epoch  14 ----\n",
            "Loss = 1.939562717760392 , Training Acc = 0.4092541436464088\n",
            "Validation Accurary =  0.3108779679734953\n",
            "---- Epoch  15 ----\n",
            "Loss = 1.9012599006175657 , Training Acc = 0.41823204419889504\n",
            "Validation Accurary =  0.334621755935947\n",
            "---- Epoch  16 ----\n",
            "Loss = 1.866736970965244 , Training Acc = 0.4334254143646409\n",
            "Validation Accurary =  0.35173937051352844\n",
            "---- Epoch  17 ----\n",
            "Loss = 1.8093281810156643 , Training Acc = 0.44613259668508287\n",
            "Validation Accurary =  0.3202650469353948\n",
            "---- Epoch  18 ----\n",
            "Loss = 1.7801117573068217 , Training Acc = 0.4562154696132596\n",
            "Validation Accurary =  0.3418001104362231\n",
            "---- Epoch  19 ----\n",
            "Loss = 1.7546482409491286 , Training Acc = 0.4595303867403315\n",
            "Validation Accurary =  0.33296521258972944\n",
            "---- Epoch  20 ----\n",
            "Loss = 1.7167306898142842 , Training Acc = 0.4725138121546962\n",
            "Validation Accurary =  0.353395913859746\n",
            "---- Epoch  21 ----\n",
            "Loss = 1.6840870118865903 , Training Acc = 0.4828729281767956\n",
            "Validation Accurary =  0.35726118166758697\n",
            "---- Epoch  22 ----\n",
            "Loss = 1.6425824741866246 , Training Acc = 0.4937845303867403\n",
            "Validation Accurary =  0.3522915516289343\n",
            "---- Epoch  23 ----\n",
            "Loss = 1.6042953401297722 , Training Acc = 0.5030386740331492\n",
            "Validation Accurary =  0.31860850358917725\n",
            "---- Epoch  24 ----\n",
            "Loss = 1.5772219754244412 , Training Acc = 0.5080110497237569\n",
            "Validation Accurary =  0.36112644947542794\n",
            "---- Epoch  25 ----\n",
            "Loss = 1.5326047092519697 , Training Acc = 0.5222375690607735\n",
            "Validation Accurary =  0.3649917172832689\n",
            "---- Epoch  26 ----\n",
            "Loss = 1.4956014557297008 , Training Acc = 0.530524861878453\n",
            "Validation Accurary =  0.3699613473219216\n",
            "---- Epoch  27 ----\n",
            "Loss = 1.4602187556142363 , Training Acc = 0.5433701657458564\n",
            "Validation Accurary =  0.3462175593594699\n",
            "---- Epoch  28 ----\n",
            "Loss = 1.4253355409373747 , Training Acc = 0.5589779005524862\n",
            "Validation Accurary =  0.3743787962451684\n",
            "---- Epoch  29 ----\n",
            "Loss = 1.395346214422353 , Training Acc = 0.5705801104972376\n",
            "Validation Accurary =  0.3506350082827167\n",
            "---- Epoch  30 ----\n",
            "Loss = 1.368610950655547 , Training Acc = 0.5707182320441989\n",
            "Validation Accurary =  0.372170071783545\n",
            "---- Epoch  31 ----\n",
            "Loss = 1.338409836428857 , Training Acc = 0.5792817679558011\n",
            "Validation Accurary =  0.353395913859746\n",
            "---- Epoch  32 ----\n",
            "Loss = 1.3119878684564998 , Training Acc = 0.5911602209944752\n",
            "Validation Accurary =  0.37824406405300937\n",
            "---- Epoch  33 ----\n",
            "Loss = 1.2909534528710052 , Training Acc = 0.5953038674033149\n",
            "Validation Accurary =  0.35394809497515184\n",
            "---- Epoch  34 ----\n",
            "Loss = 1.2606429640202446 , Training Acc = 0.6087016574585634\n",
            "Validation Accurary =  0.3627829928216455\n",
            "---- Epoch  35 ----\n",
            "Loss = 1.2389258897768767 , Training Acc = 0.6100828729281768\n",
            "Validation Accurary =  0.37161789066813916\n",
            "---- Epoch  36 ----\n",
            "Loss = 1.2021532997036637 , Training Acc = 0.620303867403315\n",
            "Validation Accurary =  0.3462175593594699\n",
            "---- Epoch  37 ----\n",
            "Loss = 1.1480355235478508 , Training Acc = 0.6382596685082873\n",
            "Validation Accurary =  0.37990060739922693\n",
            "---- Epoch  38 ----\n",
            "Loss = 1.1426868850266598 , Training Acc = 0.6410220994475139\n",
            "Validation Accurary =  0.38321369409166206\n",
            "---- Epoch  39 ----\n",
            "Loss = 1.103689527973707 , Training Acc = 0.6483425414364641\n",
            "Validation Accurary =  0.37769188293760353\n",
            "---- Epoch  40 ----\n",
            "Loss = 1.0615816653102295 , Training Acc = 0.6610497237569061\n",
            "Validation Accurary =  0.3804527885146328\n",
            "---- Epoch  41 ----\n",
            "Loss = 1.0435362949219127 , Training Acc = 0.6664364640883977\n",
            "Validation Accurary =  0.36112644947542794\n",
            "---- Epoch  42 ----\n",
            "Loss = 1.0160994618545987 , Training Acc = 0.6745856353591161\n",
            "Validation Accurary =  0.3754831584759801\n",
            "---- Epoch  43 ----\n",
            "Loss = 0.9958095726144712 , Training Acc = 0.6878453038674033\n",
            "Validation Accurary =  0.38210933186085033\n",
            "---- Epoch  44 ----\n",
            "Loss = 0.9771763535894035 , Training Acc = 0.6928176795580111\n",
            "Validation Accurary =  0.37106570955273327\n",
            "---- Epoch  45 ----\n",
            "Loss = 0.9592608461418952 , Training Acc = 0.6961325966850829\n",
            "Validation Accurary =  0.3771397018221977\n",
            "---- Epoch  46 ----\n",
            "Loss = 0.9433398888394015 , Training Acc = 0.7023480662983426\n",
            "Validation Accurary =  0.36830480397570403\n",
            "---- Epoch  47 ----\n",
            "Loss = 0.9154230102719294 , Training Acc = 0.7104972375690607\n",
            "Validation Accurary =  0.34732192159028163\n",
            "---- Epoch  48 ----\n",
            "Loss = 0.8758501687963887 , Training Acc = 0.7179558011049723\n",
            "Validation Accurary =  0.35615681943677524\n",
            "---- Epoch  49 ----\n",
            "Loss = 0.8297792534768511 , Training Acc = 0.7400552486187846\n",
            "Validation Accurary =  0.3865267807840972\n",
            "---- Epoch  50 ----\n",
            "Loss = 0.7872661365618298 , Training Acc = 0.7524861878453039\n",
            "Validation Accurary =  0.3942573163997791\n",
            "---- Epoch  51 ----\n",
            "Loss = 0.7531234159883248 , Training Acc = 0.7646408839779008\n",
            "Validation Accurary =  0.39480949751518496\n",
            "---- Epoch  52 ----\n",
            "Loss = 0.7306162387001037 , Training Acc = 0.7656077348066298\n",
            "Validation Accurary =  0.3992269464384318\n",
            "---- Epoch  53 ----\n",
            "Loss = 0.7088994090292294 , Training Acc = 0.7762430939226519\n",
            "Validation Accurary =  0.3865267807840972\n",
            "---- Epoch  54 ----\n",
            "Loss = 0.6960034321205909 , Training Acc = 0.7828729281767955\n",
            "Validation Accurary =  0.38542241855328546\n",
            "---- Epoch  55 ----\n",
            "Loss = 0.6519421975083284 , Training Acc = 0.8005524861878454\n",
            "Validation Accurary =  0.37603533959138596\n",
            "---- Epoch  56 ----\n",
            "Loss = 0.6211593201840914 , Training Acc = 0.8070441988950275\n",
            "Validation Accurary =  0.372170071783545\n",
            "---- Epoch  57 ----\n",
            "Loss = 0.6186808774881752 , Training Acc = 0.8070441988950279\n",
            "Validation Accurary =  0.36609607951408063\n",
            "---- Epoch  58 ----\n",
            "Loss = 0.6106555479770085 , Training Acc = 0.81146408839779\n",
            "Validation Accurary =  0.3804527885146328\n",
            "---- Epoch  59 ----\n",
            "Loss = 0.5848322687317742 , Training Acc = 0.8201657458563537\n",
            "Validation Accurary =  0.3826615129762562\n",
            "---- Epoch  60 ----\n",
            "Loss = 0.5563238675859926 , Training Acc = 0.8332872928176797\n",
            "Validation Accurary =  0.3793484262838211\n",
            "---- Epoch  61 ----\n",
            "Loss = 0.5163477421994347 , Training Acc = 0.8440607734806632\n",
            "Validation Accurary =  0.37272225289895083\n",
            "---- Epoch  62 ----\n",
            "Loss = 0.4957235412021603 , Training Acc = 0.8524861878453039\n",
            "Validation Accurary =  0.3699613473219216\n",
            "---- Epoch  63 ----\n",
            "Loss = 0.49376574897192943 , Training Acc = 0.849585635359116\n",
            "Validation Accurary =  0.38100496963003866\n",
            "---- Epoch  64 ----\n",
            "Loss = 0.48958639619468874 , Training Acc = 0.8486187845303869\n",
            "Validation Accurary =  0.40419657647708446\n",
            "---- Epoch  65 ----\n",
            "Loss = 0.4575607900903988 , Training Acc = 0.8609116022099449\n",
            "Validation Accurary =  0.37051352843732743\n",
            "---- Epoch  66 ----\n",
            "Loss = 0.43319868356195707 , Training Acc = 0.8697513812154697\n",
            "Validation Accurary =  0.4008834897846494\n",
            "---- Epoch  67 ----\n",
            "Loss = 0.41448272606311914 , Training Acc = 0.8776243093922653\n",
            "Validation Accurary =  0.39757040309221425\n",
            "---- Epoch  68 ----\n",
            "Loss = 0.3825667250073559 , Training Acc = 0.8890883977900553\n",
            "Validation Accurary =  0.38542241855328546\n",
            "---- Epoch  69 ----\n",
            "Loss = 0.3674840041559955 , Training Acc = 0.8970994475138122\n",
            "Validation Accurary =  0.39480949751518496\n",
            "---- Epoch  70 ----\n",
            "Loss = 0.34967272159518525 , Training Acc = 0.9006906077348068\n",
            "Validation Accurary =  0.3937051352843733\n",
            "---- Epoch  71 ----\n",
            "Loss = 0.3368010694718363 , Training Acc = 0.9091160220994475\n",
            "Validation Accurary =  0.39757040309221425\n",
            "---- Epoch  72 ----\n",
            "Loss = 0.3181124393239327 , Training Acc = 0.9168508287292818\n",
            "Validation Accurary =  0.4003313086692435\n",
            "---- Epoch  73 ----\n",
            "Loss = 0.3007215267015467 , Training Acc = 0.9243093922651935\n",
            "Validation Accurary =  0.4003313086692435\n",
            "---- Epoch  74 ----\n",
            "Loss = 0.28378558655053154 , Training Acc = 0.9281767955801105\n",
            "Validation Accurary =  0.3937051352843733\n",
            "---- Epoch  75 ----\n",
            "Loss = 0.26516001192999716 , Training Acc = 0.934254143646409\n",
            "Validation Accurary =  0.4053009387078962\n",
            "---- Epoch  76 ----\n",
            "Loss = 0.2476084337738292 , Training Acc = 0.9392265193370166\n",
            "Validation Accurary =  0.4030922142462728\n",
            "---- Epoch  77 ----\n",
            "Loss = 0.23075086959343633 , Training Acc = 0.9480662983425415\n",
            "Validation Accurary =  0.4108227498619547\n",
            "---- Epoch  78 ----\n",
            "Loss = 0.21632082958165672 , Training Acc = 0.9522099447513813\n",
            "Validation Accurary =  0.405853119823302\n",
            "---- Epoch  79 ----\n",
            "Loss = 0.2072970900547803 , Training Acc = 0.9537292817679558\n",
            "Validation Accurary =  0.4086140254003313\n",
            "---- Epoch  80 ----\n",
            "Loss = 0.19545311201172758 , Training Acc = 0.960635359116022\n",
            "Validation Accurary =  0.417448923246825\n",
            "---- Epoch  81 ----\n",
            "Loss = 0.18557545513381013 , Training Acc = 0.9629834254143647\n",
            "Validation Accurary =  0.41192711209276645\n",
            "---- Epoch  82 ----\n",
            "Loss = 0.1735603245969449 , Training Acc = 0.9690607734806632\n",
            "Validation Accurary =  0.4108227498619547\n",
            "---- Epoch  83 ----\n",
            "Loss = 0.17049536402094037 , Training Acc = 0.9686464088397789\n",
            "Validation Accurary =  0.4157923799006074\n",
            "---- Epoch  84 ----\n",
            "Loss = 0.1627441931022307 , Training Acc = 0.9708563535911603\n",
            "Validation Accurary =  0.4152401987852015\n",
            "---- Epoch  85 ----\n",
            "Loss = 0.1533050529658947 , Training Acc = 0.9716850828729282\n",
            "Validation Accurary =  0.41634456101601325\n",
            "---- Epoch  86 ----\n",
            "Loss = 0.1450748577157368 , Training Acc = 0.9755524861878454\n",
            "Validation Accurary =  0.41137493097736055\n",
            "---- Epoch  87 ----\n",
            "Loss = 0.1370148276579723 , Training Acc = 0.9777624309392264\n",
            "Validation Accurary =  0.4152401987852015\n",
            "---- Epoch  88 ----\n",
            "Loss = 0.13235253506021263 , Training Acc = 0.9812154696132597\n",
            "Validation Accurary =  0.41910546659304254\n",
            "---- Epoch  89 ----\n",
            "Loss = 0.12873261662509974 , Training Acc = 0.981353591160221\n",
            "Validation Accurary =  0.4108227498619547\n",
            "---- Epoch  90 ----\n",
            "Loss = 0.1248103338101374 , Training Acc = 0.9828729281767956\n",
            "Validation Accurary =  0.41137493097736055\n",
            "---- Epoch  91 ----\n",
            "Loss = 0.12079256882095389 , Training Acc = 0.9838397790055248\n",
            "Validation Accurary =  0.4251794588625069\n",
            "---- Epoch  92 ----\n",
            "Loss = 0.11469269001773692 , Training Acc = 0.986878453038674\n",
            "Validation Accurary =  0.41855328547763665\n",
            "---- Epoch  93 ----\n",
            "Loss = 0.11138005984435405 , Training Acc = 0.9861878453038674\n",
            "Validation Accurary =  0.417448923246825\n",
            "---- Epoch  94 ----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SFZe8By_D997"
      }
    }
  ]
}